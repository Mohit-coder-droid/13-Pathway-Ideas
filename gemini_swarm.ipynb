{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mohit-coder-droid/13-Pathway-Ideas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92M_Nr06dAAl",
        "outputId": "b73295fc-e3fe-4513-9c3e-6db93dbdb6a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '13-Pathway-Ideas'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UYWA2m7cOTIF"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
      ],
      "metadata": {
        "id": "OKCh7Pq_PuO7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking how to incorporate google gemini model on swarm"
      ],
      "metadata": {
        "id": "Q4DCPfc8U1ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import copy\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from typing import List, Callable, Union\n",
        "\n",
        "# Package/library imports\n",
        "from typing import List, Callable, Union, Optional\n",
        "\n",
        "# Third-party imports\n",
        "from pydantic import BaseModel\n",
        "AgentFunction = Callable[[], Union[str, \"Agent\", dict]]"
      ],
      "metadata": {
        "id": "d5zeIz49PqHa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(1, '/content/13-Pathway-Ideas')"
      ],
      "metadata": {
        "id": "EdKLO-UGdbuD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local imports\n",
        "from util import function_to_json, debug_print, merge_chunk"
      ],
      "metadata": {
        "id": "5eejJd0UVZyG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function which takes nothing but returns either string, Agent, or dict\n",
        "AgentFunction = Callable[[], Union[str, \"Agent\", dict]]"
      ],
      "metadata": {
        "id": "3mt_NJjp8ph5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(BaseModel):\n",
        "    name: str = \"Agent\"\n",
        "    model: str = \"gemini-1.5-flash\"\n",
        "    instructions: Union[str, Callable[[], str]] = \"You are a helpful agent.\"\n",
        "    functions: List[AgentFunction] = []\n",
        "    tool_choice: str = None\n",
        "    parallel_tool_calls: bool = True\n",
        "\n",
        "\n",
        "class Response(BaseModel):\n",
        "    messages: List = []\n",
        "    agent: Optional[Agent] = None\n",
        "    context_variables: dict = {}\n",
        "\n",
        "class Result(BaseModel):\n",
        "    \"\"\"\n",
        "    Encapsulates the possible return values for an agent function.\n",
        "\n",
        "    Attributes:\n",
        "        value (str): The result value as a string.\n",
        "        agent (Agent): The agent instance, if applicable.\n",
        "        context_variables (dict): A dictionary of context variables.\n",
        "    \"\"\"\n",
        "\n",
        "    value: str = \"\"\n",
        "    agent: Optional[Agent] = None\n",
        "    context_variables: dict = {}"
      ],
      "metadata": {
        "id": "4K_H86mjVTzf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this code I am not dealing with the context variables to keep things simple and because of lack of time\n",
        "# I am also not tackling the stream part\n",
        "# In my code the model gets only one chance to say something but in the original one, it takes multiple iteration and functions calling to get to the right result\n",
        "__CTX_VARS_NAME__ = \"context_variables\""
      ],
      "metadata": {
        "id": "TH3zRZeeVl0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## genai.GenerativeModel()"
      ],
      "metadata": {
        "id": "eUW0Xxsm3ssM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Swarm:\n",
        "    def __init__(self):\n",
        "        self.client = genai.GenerativeModel()\n",
        "\n",
        "    def get_chat_completion(\n",
        "            self,\n",
        "            agent:Agent,\n",
        "            history:List,\n",
        "            context_variables: dict,\n",
        "            model_override: str,\n",
        "            stream: bool,\n",
        "            debug: bool,\n",
        "            max_history:int = 10,  # max amount of history that the model can see\n",
        "    ):\n",
        "        context_variables = defaultdict(str, context_variables)\n",
        "\n",
        "        instructions = (\n",
        "            agent.instructions(context_variables)\n",
        "            if callable(agent.instructions)\n",
        "            else agent.instructions\n",
        "        )\n",
        "\n",
        "        messages = [{\"role\": \"system\", \"content\": instructions}]\n",
        "\n",
        "        if history[-1]:\n",
        "            if \"role\" in history[-1] and history[-1]['role'] == \"model\":\n",
        "                messages.append(f\"In the following conversation your last response was {history[-1]}. So, consider this and following conversation and tell your response. \")\n",
        "\n",
        "        if len(history)>max_history:\n",
        "            messages.append(history[-max_history:])\n",
        "        else:\n",
        "            messages.append( history)\n",
        "        debug_print(debug, \"Getting chat completion for...:\", messages)\n",
        "\n",
        "\n",
        "        # I have to do something to include context_variables here\n",
        "\n",
        "\n",
        "\n",
        "        # In google generative ai, we just have to pass the name of the function\n",
        "        create_params = {\n",
        "            \"model_name\": model_override or agent.model,\n",
        "            \"tools\": agent.functions,\n",
        "        }\n",
        "\n",
        "        self.client = genai.GenerativeModel(**create_params)\n",
        "        return self.client.generate_content(str(messages))\n",
        "        # return self.client.generate_content(\"Can you tell me what things can you do\")\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        agent: Agent,\n",
        "        messages: List,\n",
        "        context_variables: dict = {},\n",
        "        model_override: str = None,\n",
        "        stream: bool = False,\n",
        "        debug: bool = False,\n",
        "        max_turns: int = float(\"inf\"),\n",
        "        max_history:int = 10,  # max amount of history that the model can see\n",
        "        execute_tools: bool = True,\n",
        "    ):\n",
        "        active_agent = agent\n",
        "        context_variables = copy.deepcopy(context_variables)\n",
        "        history = copy.deepcopy(messages)\n",
        "        init_len = len(messages)\n",
        "\n",
        "        n = 1\n",
        "\n",
        "        # An agent can be called multiple times after implementing a function\n",
        "        # while len(history) - init_len < max_turns and active_agent:\n",
        "\n",
        "        # get completion with current history, agent\n",
        "        completion = self.get_chat_completion(\n",
        "            agent=active_agent,\n",
        "            history=history,\n",
        "            context_variables=context_variables,\n",
        "            model_override=model_override,\n",
        "            stream=stream,\n",
        "            debug=debug,\n",
        "            max_history=max_history\n",
        "        )\n",
        "\n",
        "\n",
        "        # For now I am making it so that it's able to call only one function\n",
        "        message = completion.to_dict()['candidates'][0]['content']\n",
        "        message['sender'] = active_agent.name\n",
        "        debug_print(debug, \"Received completion:\", message)\n",
        "\n",
        "        # if not message.function_call or not execute_tools:\n",
        "        history.append(message)\n",
        "\n",
        "        # if there is no function calling then break\n",
        "        if \"function_call\" not in message['parts'][0] or not execute_tools:\n",
        "            debug_print(debug, \"Ending turn.\")\n",
        "            return Response(\n",
        "                messages=history[init_len:],\n",
        "                # messages=[message],\n",
        "                agent=active_agent,\n",
        "                context_variables=context_variables,\n",
        "            )\n",
        "\n",
        "\n",
        "        # handle function calls, updating context_variables, and switching agents\n",
        "        partial_response,history = self.handle_function_calls(\n",
        "            message['parts'], functions = active_agent.functions,context_variables = context_variables, debug=debug,history=history\n",
        "        )\n",
        "        history.extend(partial_response.messages)\n",
        "\n",
        "        context_variables.update(partial_response.context_variables)\n",
        "        if partial_response.agent:\n",
        "            active_agent = partial_response.agent\n",
        "\n",
        "        return Response(\n",
        "            messages=history[init_len:],\n",
        "            # messages=[message],\n",
        "            agent=active_agent,\n",
        "            context_variables=context_variables,\n",
        "        )\n",
        "\n",
        "    # Handle the function result\n",
        "    def handle_function_result(self, result, debug) -> Result:\n",
        "        match result:\n",
        "            case Result() as result:\n",
        "                return result\n",
        "\n",
        "            case Agent() as agent:\n",
        "                return Result(\n",
        "                    value=json.dumps({\"assistant\": agent.name}),\n",
        "                    agent=agent,\n",
        "                )\n",
        "            case _:\n",
        "                try:\n",
        "                    return Result(value=str(result))\n",
        "                except Exception as e:\n",
        "                    error_message = f\"Failed to cast response to string: {result}. Make sure agent functions return a string or Result object. Error: {str(e)}\"\n",
        "                    debug_print(debug, error_message)\n",
        "                    raise TypeError(error_message)\n",
        "\n",
        "\n",
        "    # For now implementing only for single function call, also check for context_variables\n",
        "\n",
        "\n",
        "\n",
        "    def handle_function_calls(\n",
        "        self,\n",
        "        function_call,\n",
        "        functions,\n",
        "        context_variables:dict,\n",
        "        debug:bool,\n",
        "        history\n",
        "    ) -> Response:\n",
        "        function_map = {f.__name__: f for f in functions}\n",
        "        partial_response = Response(\n",
        "            messages=[], agent=None, context_variables={})\n",
        "\n",
        "        for func in function_call:\n",
        "            name = func['function_call']['name']\n",
        "            args = func['function_call']['args']\n",
        "\n",
        "            # If our model has done mistake while calling the function_name\n",
        "            if name not in function_map:\n",
        "                debug_print(debug, f\"Tool {name} not found in function map.\")\n",
        "                partial_response.messages.append(\n",
        "                    {\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_name\": name,\n",
        "                        \"content\": f\"Error: Tool {name} not found.\",\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                continue\n",
        "\n",
        "        debug_print(\n",
        "                debug, f\"Processing tool call: {name} with arguments {args}\")\n",
        "\n",
        "        func = function_map[name]\n",
        "\n",
        "        # If there is something which the user hadn't mentioned thinking that the model already know that then it must be included in context_var\n",
        "        # pass context_variables to agent functions\n",
        "        # if __CTX_VARS_NAME__ in func.__code__.co_varnames:\n",
        "        #     raw_result = functions[function_name](__CTX_VARS_NAME__,**function_args)\n",
        "        # else:\n",
        "\n",
        "        raw_result = func(**args)\n",
        "        # print(\"No Problem\")\n",
        "        result: Result = self.handle_function_result(raw_result, debug)\n",
        "\n",
        "        # Also check what it tool_id\n",
        "        partial_response.messages.append(\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"tool_name\": name,\n",
        "                \"content\": result.value,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        partial_response.context_variables.update(result.context_variables)\n",
        "        if result.agent:\n",
        "            partial_response.agent = result.agent\n",
        "\n",
        "            create_params = {\n",
        "                \"model_name\": result.agent.model,\n",
        "            }\n",
        "\n",
        "            self.client = genai.GenerativeModel(**create_params)\n",
        "            message =  self.client.generate_content(\"Introduce yourself to user in just one line strictly. \" + result.agent.instructions).to_dict()['candidates'][0]['content']\n",
        "\n",
        "            # For now I am making it so that it's able to call only one function\n",
        "            message['sender'] = result.agent.name\n",
        "\n",
        "            # if not message.function_call or not execute_tools:\n",
        "            history.append(message)\n",
        "\n",
        "\n",
        "        return partial_response,history\n"
      ],
      "metadata": {
        "id": "PTeHSLlXyLiG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_demo_loop(\n",
        "    starting_agent, context_variables=None, stream=False, debug=False\n",
        ") -> None:\n",
        "    client = Swarm()\n",
        "    print(\"Starting Swarm CLI üêù\")\n",
        "\n",
        "    messages = []\n",
        "    agent = starting_agent\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"User Input: \")\n",
        "        if user_input.lower() == \"/exit\":\n",
        "          print(\"Exiting the loop. Goodbye!\")\n",
        "          break  # Exit the loop\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        response = client.run(\n",
        "            agent=agent,\n",
        "            messages=messages,\n",
        "            context_variables=context_variables or {},\n",
        "            stream=stream,\n",
        "            debug=debug,\n",
        "        )\n",
        "\n",
        "        # return response\n",
        "        # print(response)\n",
        "\n",
        "        pretty_print_messages(response.messages)\n",
        "\n",
        "        messages.extend(response.messages)\n",
        "        agent = response.agent\n",
        "\n",
        "\n",
        "\n",
        "def pretty_print_messages(messages) -> None:\n",
        "    for message in messages:\n",
        "\n",
        "        if ('role' in message and (message['role']=='model' or message['role']=='assisstant')):\n",
        "            # print agent name in blue\n",
        "            print(f\"\\033[94m{message['sender']}\\033[0m:\", end=\" \")\n",
        "\n",
        "            if 'parts' in message:\n",
        "                for msg in message['parts']:\n",
        "                    if 'text' in msg:\n",
        "                        print(msg['text'])\n",
        "\n",
        "                    if 'function_call' in msg:\n",
        "                        print(f\"{msg['function_call']['name']}()\")\n"
      ],
      "metadata": {
        "id": "1ygzm8Iqdvi1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cricket App:-"
      ],
      "metadata": {
        "id": "lvf4Mp5lpi-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions:-"
      ],
      "metadata": {
        "id": "kz9VWLNa4sMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's not working properly maybe because of it's complexity the model is giving wrong outputs. so I didn't tested it fully\n",
        "\n",
        "This is just the architecture of my swarm agents"
      ],
      "metadata": {
        "id": "IXhBFVXbEYKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def player_detail(name:str,team:str)->dict:\n",
        "    \"\"\"\n",
        "        Returns a brief overview of about any player in the form of json, what is his performance since last month, what is his overall performance, performance in this tournamnt\n",
        "\n",
        "        Args:\n",
        "            name: Name of the player\n",
        "            team: Team which player belongs to\n",
        "    \"\"\"\n",
        "\n",
        "    player_detail = {\n",
        "        name:\"Mahendra Singh Dhoni\",\n",
        "        team:\"Chennai Super King\",\n",
        "        overall_performance:[90,100,110,23],\n",
        "        total_sixes:230,\n",
        "        total_fours:500,\n",
        "        performance_ipl_wise:{\"2023\":\"Avergage 230 runs in all matches\"}\n",
        "    }\n",
        "\n",
        "    return player_detail\n",
        "\n",
        "def player_comparision(player1:str,player2:str)->str:\n",
        "    \"\"\"\n",
        "        Returns a comparision between two players\n",
        "    \"\"\"\n",
        "\n",
        "    return \"Virat Kohli is better than Rohit Sharma\"\n",
        "\n",
        "def probability_winning(team1:dict,team2:dict)->str:\n",
        "    \"\"\"Calculates the probability of winning of a match between two teams\"\"\"\n",
        "\n",
        "    return \"CSK will win against Mumbai Indians by 130 runs\"\n",
        "\n",
        "def get_weather(location:str,time:str=\"now\"):\n",
        "    \"\"\"Returns weather Forecasting of any location for any time.\"\"\"\n",
        "\n",
        "    return \"It will be a sunny day\"\n",
        "\n",
        "def live_match(match:str)->dict:\n",
        "    \"\"\"Gives live detail about a match\"\"\"\n",
        "\n",
        "    return {\"batting team\":{\n",
        "        \"run\":200,\n",
        "        \"wickets\":3,\n",
        "        \"batters\":[\"Virat Kohli\",\"Suryakumar\"],\n",
        "        \"remaining balls\":10\n",
        "    }}\n",
        "\n",
        "def get_news():\n",
        "    \"\"\"Returns latest news of cricket\"\"\"\n",
        "\n",
        "    return \"CSK won the yesterday match\"\n",
        "\n",
        "def book_ticket(match:str,day:str,location:str):\n",
        "    \"\"\"Book tickets for match that is going to happen\"\"\"\n",
        "\n",
        "    return \"Your ticket has been booked\"\n",
        "\n",
        "def cancel_ticket(booking_id:int):\n",
        "    \"\"\"Cancels the ticket which already has been booked\"\"\"\n",
        "\n",
        "    return \"Your ticket has been cancelled\""
      ],
      "metadata": {
        "id": "5N1eOwnT4uDQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer functions:-"
      ],
      "metadata": {
        "id": "vHDCXos5E1Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_to_player():\n",
        "    \"\"\"Call this function when the player_agent() => [gives information about player] seems suitable for handling the task\"\"\"\n",
        "    return player_agent\n",
        "\n",
        "def transfer_to_analytic():\n",
        "    \"\"\"Call this function when the analytic_agent() => [Do some analysis] seems suitable for handling the task\"\"\"\n",
        "    return analytic_agent\n",
        "\n",
        "def transfer_to_live():\n",
        "    \"\"\"Call this function when the live_agent() => [Gives live information about match] seems suitable for handling the task\"\"\"\n",
        "    return live_agent\n",
        "\n",
        "def transfer_to_book():\n",
        "    \"\"\"Call this function when the book_agent() => [Book and cancel ticket for matches] seems suitable for handling the task\"\"\"\n",
        "    return book_agent\n",
        "\n",
        "def transfer_to_triage():\n",
        "    \"\"\"Call this function when a user needs to be transferred to a differnt agent and a different policy.\n",
        "    For instance, if a user is asking about a topic that is not handled by the current agent, call this function.\n",
        "    \"\"\"\n",
        "    return triage_agent\n",
        "\n",
        "Start_prompt = \"\"\"\n",
        "    You are an intelligent and empathetic customer support representative for cricket app. Your task is to call the function that can perform the task asked by user very well. Don't hesitate to ask the user again if you think that sufficient information is not given about the task.\n",
        "    Strictly don't assume anything from your side and don't hallucinate.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "end_prompt = \"\"\"\n",
        "    NOTE: If the user requests for something that you can't tackle then call 'transfer_to_triage' function strictly\n",
        "\"\"\"\n",
        "\n",
        "triage_agent_instruc = \"\"\"\n",
        "    You are an expert triaging agent for cricket app.\n",
        "    You have to transfer the task to the right agent\"\"\""
      ],
      "metadata": {
        "id": "ESDS9ghhE3Za"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agents:-"
      ],
      "metadata": {
        "id": "PO_7QCNY4iTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "player_agent = Agent(\n",
        "    name = \"Player Agent\",\n",
        "    instructions =Start_prompt +  \"Your task is to give information about some player, his performance in past matches, his upcoming matches schedule\" + end_prompt,\n",
        "    tools = [player_detail,transfer_to_triage]\n",
        ")\n",
        "\n",
        "analytic_agent = Agent(\n",
        "    name = \"Analytics Agent\",\n",
        "    instructions = Start_prompt + \"You can perform any kind of analytical task, like finding out probability which team will win, which player is likely to perform well in the upcoming matches\" + end_prompt,\n",
        "    tools = [player_comparision,probability_winning,transfer_to_triage]\n",
        ")\n",
        "\n",
        "live_agent = Agent(\n",
        "    name = \"Live Agent\",\n",
        "    instructions = Start_prompt + \"Your task is to give live details about any match. \" + end_prompt,\n",
        "    tools = [live_match,get_weather,get_news,transfer_to_triage]\n",
        "\n",
        ")\n",
        "\n",
        "book_agent = Agent(\n",
        "    name=\"Book Agent\",\n",
        "    instructions = Start_prompt + \"Your task is to book ticket for matches. \" + end_prompt,\n",
        "    tools = [book_ticket,cancel_ticket,transfer_to_triage]\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name = \"Triage Agent\",\n",
        "    instructions = triage_agent_instruc,\n",
        "    tools = [transfer_to_player,transfer_to_analytic,transfer_to_live,transfer_to_book]\n",
        ")"
      ],
      "metadata": {
        "id": "cN_EAWCuplcd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_demo_loop(triage_agent,debug=True)  # here it should have returned role as model but now it's giving role as system, before this run it return role as assisstant. So, it's not working very properly here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "c00B77-mH_4I",
        "outputId": "4047a6fc-9d1e-4536-f542-5419794822da"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Swarm CLI üêù\n",
            "User Input: Hello there\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:06:15\u001b[97m]\u001b[90m Getting chat completion for...: [{'role': 'system', 'content': 'You are a model and You have to transfer the task to the right agent'}, [{'role': 'user', 'content': 'Hello there'}]]\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:06:18\u001b[97m]\u001b[90m Received completion: {'parts': [{'text': '```json\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"This is a simple greeting. No specific task is required.\"\\n  }\\n]\\n```'}], 'role': 'model', 'sender': 'Triage Agent'}\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:06:18\u001b[97m]\u001b[90m Ending turn.\u001b[0m\n",
            "\u001b[94mTriage Agent\u001b[0m: ```json\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"This is a simple greeting. No specific task is required.\"\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d6ee6c9ff9a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_demo_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriage_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-e9ae275f0d51>\u001b[0m in \u001b[0;36mrun_demo_loop\u001b[0;34m(starting_agent, context_variables, stream, debug)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User Input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"/exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting the loop. Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For checking the Working of Gemini-Swarm:"
      ],
      "metadata": {
        "id": "SD6eDY7hd2BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weather Agent:-"
      ],
      "metadata": {
        "id": "xx9rSqJgd7G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather(location:str, time:str=\"now\"):\n",
        "    \"\"\"Get the current weather in a given location.\n",
        "\n",
        "        Args:\n",
        "            location: City or state\n",
        "    \"\"\"\n",
        "    print(f\"Weather at {location} will be sunny today\")\n",
        "\n",
        "\n",
        "def send_email(recipient:str, subject:str, body:str):\n",
        "    \"\"\"\n",
        "        For sending email to recipient\n",
        "        Args:\n",
        "            recipient: Receiver of email\n",
        "            subject: subject of email\n",
        "            body: body of email\n",
        "    \"\"\"\n",
        "    print(\"Sending email...\")\n",
        "    print(f\"To: {recipient}\")\n",
        "    print(f\"Subject: {subject}\")\n",
        "    print(f\"Body: {body}\")\n",
        "    return \"Sent!\"\n",
        "\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"Weather Agent\",\n",
        "    instructions=\"You are a helpful agent. Ask questions from user if you think that there is some missing information in the query instead of directly calling the function\",\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    functions=[get_weather, send_email],\n",
        ")"
      ],
      "metadata": {
        "id": "xPnZghq-eAsc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_demo_loop(weather_agent,debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "6wB3J2IZIPWG",
        "outputId": "4fe3ba22-4467-4571-a920-6894f4bbf61d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Swarm CLI üêù\n",
            "User Input: Hi there\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:32\u001b[97m]\u001b[90m Getting chat completion for...: [{'role': 'system', 'content': 'You are a helpful agent. Ask questions from user if you think that there is some missing information in the query instead of directly calling the function'}, [{'role': 'user', 'content': 'Hi there'}]]\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:36\u001b[97m]\u001b[90m Received completion: {'parts': [{'text': 'What can I help you with today? \\n'}], 'role': 'model', 'sender': 'Weather Agent'}\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:36\u001b[97m]\u001b[90m Ending turn.\u001b[0m\n",
            "\u001b[94mWeather Agent\u001b[0m: What can I help you with today? \n",
            "\n",
            "User Input: I want to know the weather at Patna\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:44\u001b[97m]\u001b[90m Getting chat completion for...: [{'role': 'system', 'content': 'You are a helpful agent. Ask questions from user if you think that there is some missing information in the query instead of directly calling the function'}, [{'role': 'user', 'content': 'Hi there'}, {'parts': [{'text': 'What can I help you with today? \\n'}], 'role': 'model', 'sender': 'Weather Agent'}, {'role': 'user', 'content': 'I want to know the weather at Patna'}]]\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:46\u001b[97m]\u001b[90m Received completion: {'parts': [{'function_call': {'name': 'get_weather', 'args': {'location': 'Patna'}}}], 'role': 'model', 'sender': 'Weather Agent'}\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:46\u001b[97m]\u001b[90m Processing tool call: get_weather with arguments {'location': 'Patna'}\u001b[0m\n",
            "Weather at Patna will be sunny today\n",
            "\u001b[94mWeather Agent\u001b[0m: get_weather()\n",
            "User Input: Send an email to mohit@gmail.com with subject as application for leave and write that I want leave because I am sick\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:54\u001b[97m]\u001b[90m Getting chat completion for...: [{'role': 'system', 'content': 'You are a helpful agent. Ask questions from user if you think that there is some missing information in the query instead of directly calling the function'}, [{'role': 'user', 'content': 'Hi there'}, {'parts': [{'text': 'What can I help you with today? \\n'}], 'role': 'model', 'sender': 'Weather Agent'}, {'role': 'user', 'content': 'I want to know the weather at Patna'}, {'parts': [{'function_call': {'name': 'get_weather', 'args': {'location': 'Patna'}}}], 'role': 'model', 'sender': 'Weather Agent'}, {'role': 'tool', 'tool_name': 'get_weather', 'content': 'None'}, {'role': 'user', 'content': 'Send an email to mohit@gmail.com with subject as application for leave and write that I want leave because I am sick'}]]\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:56\u001b[97m]\u001b[90m Received completion: {'parts': [{'function_call': {'name': 'send_email', 'args': {'recipient': 'mohit@gmail.com', 'body': 'I want leave because I am sick', 'subject': 'application for leave'}}}], 'role': 'model', 'sender': 'Weather Agent'}\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:30:56\u001b[97m]\u001b[90m Processing tool call: send_email with arguments {'recipient': 'mohit@gmail.com', 'body': 'I want leave because I am sick', 'subject': 'application for leave'}\u001b[0m\n",
            "Sending email...\n",
            "To: mohit@gmail.com\n",
            "Subject: application for leave\n",
            "Body: I want leave because I am sick\n",
            "\u001b[94mWeather Agent\u001b[0m: send_email()\n",
            "User Input: /exit\n",
            "Exiting the loop. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triage Agent"
      ],
      "metadata": {
        "id": "koIF1kEKfZQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_refund(item_id:int, reason:str=\"NOT SPECIFIED\"):\n",
        "    \"\"\"Refund an item. Refund an item. Make sure you have the item_id of the form item_... Ask for user confirmation before processing the refund.\"\"\"\n",
        "    print(f\"[mock] Refunding item {item_id} because {reason}...\")\n",
        "    return \"Success!\"\n",
        "\n",
        "\n",
        "def apply_discount():\n",
        "    \"\"\"Apply a discount to the user's cart.\"\"\"\n",
        "    print(\"[mock] Applying discount...\")\n",
        "    return \"Applied discount of 11%\"\n",
        "\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage Agent\",\n",
        "    instructions=\"Determine which agent is best suited to handle the user's request, and transfer the conversation to that agent.\",\n",
        ")\n",
        "sales_agent = Agent(\n",
        "    name=\"Sales Agent\",\n",
        "    instructions=\"Be super enthusiastic about selling bees.\",\n",
        ")\n",
        "refunds_agent = Agent(\n",
        "    name=\"Refunds Agent\",\n",
        "    instructions=\"Help the user with a refund. If the reason is that it was too expensive, offer the user a refund code. If they insist, then process the refund.\",\n",
        "    functions=[process_refund, apply_discount],\n",
        ")\n",
        "\n",
        "\n",
        "def transfer_back_to_triage():\n",
        "    \"\"\"Call this function if a user is asking about a topic that is not handled by the current agent.\"\"\"\n",
        "    return triage_agent\n",
        "\n",
        "\n",
        "def transfer_to_sales():\n",
        "    return sales_agent\n",
        "\n",
        "\n",
        "def transfer_to_refunds():\n",
        "    return refunds_agent\n",
        "\n",
        "\n",
        "triage_agent.functions = [transfer_to_sales, transfer_to_refunds]\n",
        "sales_agent.functions.append(transfer_back_to_triage)\n",
        "refunds_agent.functions.append(transfer_back_to_triage)"
      ],
      "metadata": {
        "id": "DQ8j3X9ReF1C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_demo_loop(triage_agent,debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "nJYoX3_Ofadn",
        "outputId": "4f4cf5d8-3828-4d72-f3dc-964dd0e10ee7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Swarm CLI üêù\n",
            "User Input: I want my refund\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:32:47\u001b[97m]\u001b[90m Getting chat completion for...: [{'role': 'system', 'content': \"Determine which agent is best suited to handle the user's request, and transfer the conversation to that agent.\"}, [{'role': 'user', 'content': 'I want my refund'}]]\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:32:52\u001b[97m]\u001b[90m Received completion: {'parts': [{'function_call': {'name': 'transfer_to_refunds', 'args': {}}}], 'role': 'model', 'sender': 'Triage Agent'}\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:32:52\u001b[97m]\u001b[90m Processing tool call: transfer_to_refunds with arguments {}\u001b[0m\n",
            "\u001b[94mTriage Agent\u001b[0m: transfer_to_refunds()\n",
            "\u001b[94mRefunds Agent\u001b[0m: Hello, I can help you with a refund - what's the reason for your request? \n",
            "\n",
            "User Input: I want my refund for honey whose id=123\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:33:02\u001b[97m]\u001b[90m Getting chat completion for...: [{'role': 'system', 'content': 'Help the user with a refund. If the reason is that it was too expensive, offer the user a refund code. If they insist, then process the refund.'}, [{'role': 'user', 'content': 'I want my refund'}, {'parts': [{'function_call': {'name': 'transfer_to_refunds', 'args': {}}}], 'role': 'model', 'sender': 'Triage Agent'}, {'parts': [{'text': \"Hello, I can help you with a refund - what's the reason for your request? \\n\"}], 'role': 'model', 'sender': 'Refunds Agent'}, {'role': 'tool', 'tool_name': 'transfer_to_refunds', 'content': '{\"assistant\": \"Refunds Agent\"}'}, {'role': 'user', 'content': 'I want my refund for honey whose id=123'}]]\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:33:04\u001b[97m]\u001b[90m Received completion: {'parts': [{'function_call': {'name': 'process_refund', 'args': {'reason': 'The honey was too expensive', 'item_id': '123'}}}], 'role': 'model', 'sender': 'Refunds Agent'}\u001b[0m\n",
            "\u001b[97m[\u001b[90m2024-10-21 14:33:04\u001b[97m]\u001b[90m Processing tool call: process_refund with arguments {'reason': 'The honey was too expensive', 'item_id': '123'}\u001b[0m\n",
            "[mock] Refunding item 123 because The honey was too expensive...\n",
            "\u001b[94mRefunds Agent\u001b[0m: process_refund()\n",
            "User Input: /exit\n",
            "Exiting the loop. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjYtaKFCfjxy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}